{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "['setosa' 'versicolor' 'virginica']\n",
      "150\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "\n",
    "print(iris.feature_names)\n",
    "print(iris.target_names)\n",
    "print(len(iris.data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 1.10630614\n",
      "Iteration 2, loss = 1.09360130\n",
      "Iteration 3, loss = 1.08206889\n",
      "Iteration 4, loss = 1.07173283\n",
      "Iteration 5, loss = 1.06255777\n",
      "Iteration 6, loss = 1.05441695\n",
      "Iteration 7, loss = 1.04708632\n",
      "Iteration 8, loss = 1.04028820\n",
      "Iteration 9, loss = 1.03374962\n",
      "Iteration 10, loss = 1.02724658\n",
      "Iteration 11, loss = 1.02062333\n",
      "Iteration 12, loss = 1.01379151\n",
      "Iteration 13, loss = 1.00671719\n",
      "Iteration 14, loss = 0.99940063\n",
      "Iteration 15, loss = 0.99185537\n",
      "Iteration 16, loss = 0.98409193\n",
      "Iteration 17, loss = 0.97610841\n",
      "Iteration 18, loss = 0.96788860\n",
      "Iteration 19, loss = 0.95940685\n",
      "Iteration 20, loss = 0.95063754\n",
      "Iteration 21, loss = 0.94156518\n",
      "Iteration 22, loss = 0.93219145\n",
      "Iteration 23, loss = 0.92253725\n",
      "Iteration 24, loss = 0.91264005\n",
      "Iteration 25, loss = 0.90254842\n",
      "Iteration 26, loss = 0.89231541\n",
      "Iteration 27, loss = 0.88199236\n",
      "Iteration 28, loss = 0.87162383\n",
      "Iteration 29, loss = 0.86124458\n",
      "Iteration 30, loss = 0.85087874\n",
      "Iteration 31, loss = 0.84054117\n",
      "Iteration 32, loss = 0.83024049\n",
      "Iteration 33, loss = 0.81998297\n",
      "Iteration 34, loss = 0.80977635\n",
      "Iteration 35, loss = 0.79963243\n",
      "Iteration 36, loss = 0.78956797\n",
      "Iteration 37, loss = 0.77960376\n",
      "Iteration 38, loss = 0.76976240\n",
      "Iteration 39, loss = 0.76006548\n",
      "Iteration 40, loss = 0.75053118\n",
      "Iteration 41, loss = 0.74117270\n",
      "Iteration 42, loss = 0.73199796\n",
      "Iteration 43, loss = 0.72301029\n",
      "Iteration 44, loss = 0.71420993\n",
      "Iteration 45, loss = 0.70559559\n",
      "Iteration 46, loss = 0.69716577\n",
      "Iteration 47, loss = 0.68891954\n",
      "Iteration 48, loss = 0.68085665\n",
      "Iteration 49, loss = 0.67297724\n",
      "Iteration 50, loss = 0.66528121\n",
      "Iteration 51, loss = 0.65776768\n",
      "Iteration 52, loss = 0.65043449\n",
      "Iteration 53, loss = 0.64327813\n",
      "Iteration 54, loss = 0.63629378\n",
      "Iteration 55, loss = 0.62947566\n",
      "Iteration 56, loss = 0.62281742\n",
      "Iteration 57, loss = 0.61631247\n",
      "Iteration 58, loss = 0.60995430\n",
      "Iteration 59, loss = 0.60373661\n",
      "Iteration 60, loss = 0.59765340\n",
      "Iteration 61, loss = 0.59169907\n",
      "Iteration 62, loss = 0.58586842\n",
      "Iteration 63, loss = 0.58015682\n",
      "Iteration 64, loss = 0.57456027\n",
      "Iteration 65, loss = 0.56907551\n",
      "Iteration 66, loss = 0.56370006\n",
      "Iteration 67, loss = 0.55843219\n",
      "Iteration 68, loss = 0.55327075\n",
      "Iteration 69, loss = 0.54821501\n",
      "Iteration 70, loss = 0.54326435\n",
      "Iteration 71, loss = 0.53841802\n",
      "Iteration 72, loss = 0.53367491\n",
      "Iteration 73, loss = 0.52903337\n",
      "Iteration 74, loss = 0.52449120\n",
      "Iteration 75, loss = 0.52004557\n",
      "Iteration 76, loss = 0.51569325\n",
      "Iteration 77, loss = 0.51143065\n",
      "Iteration 78, loss = 0.50725401\n",
      "Iteration 79, loss = 0.50315952\n",
      "Iteration 80, loss = 0.49914340\n",
      "Iteration 81, loss = 0.49520190\n",
      "Iteration 82, loss = 0.49133131\n",
      "Iteration 83, loss = 0.48752795\n",
      "Iteration 84, loss = 0.48378815\n",
      "Iteration 85, loss = 0.48010824\n",
      "Iteration 86, loss = 0.47648459\n",
      "Iteration 87, loss = 0.47291363\n",
      "Iteration 88, loss = 0.46939181\n",
      "Iteration 89, loss = 0.46591565\n",
      "Iteration 90, loss = 0.46248169\n",
      "Iteration 91, loss = 0.45908651\n",
      "Iteration 92, loss = 0.45572672\n",
      "Iteration 93, loss = 0.45239904\n",
      "Iteration 94, loss = 0.44910035\n",
      "Iteration 95, loss = 0.44582781\n",
      "Iteration 96, loss = 0.44257886\n",
      "Iteration 97, loss = 0.43935129\n",
      "Iteration 98, loss = 0.43614319\n",
      "Iteration 99, loss = 0.43295291\n",
      "Iteration 100, loss = 0.42977901\n",
      "Iteration 101, loss = 0.42662018\n",
      "Iteration 102, loss = 0.42347527\n",
      "Iteration 103, loss = 0.42034326\n",
      "Iteration 104, loss = 0.41722327\n",
      "Iteration 105, loss = 0.41411455\n",
      "Iteration 106, loss = 0.41101647\n",
      "Iteration 107, loss = 0.40792848\n",
      "Iteration 108, loss = 0.40485015\n",
      "Iteration 109, loss = 0.40178111\n",
      "Iteration 110, loss = 0.39872113\n",
      "Iteration 111, loss = 0.39567007\n",
      "Iteration 112, loss = 0.39262796\n",
      "Iteration 113, loss = 0.38959489\n",
      "Iteration 114, loss = 0.38657105\n",
      "Iteration 115, loss = 0.38355667\n",
      "Iteration 116, loss = 0.38055200\n",
      "Iteration 117, loss = 0.37755733\n",
      "Iteration 118, loss = 0.37457298\n",
      "Iteration 119, loss = 0.37159929\n",
      "Iteration 120, loss = 0.36863665\n",
      "Iteration 121, loss = 0.36568546\n",
      "Iteration 122, loss = 0.36274612\n",
      "Iteration 123, loss = 0.35981907\n",
      "Iteration 124, loss = 0.35690475\n",
      "Iteration 125, loss = 0.35400361\n",
      "Iteration 126, loss = 0.35111615\n",
      "Iteration 127, loss = 0.34824284\n",
      "Iteration 128, loss = 0.34538414\n",
      "Iteration 129, loss = 0.34254050\n",
      "Iteration 130, loss = 0.33971234\n",
      "Iteration 131, loss = 0.33690007\n",
      "Iteration 132, loss = 0.33410409\n",
      "Iteration 133, loss = 0.33132478\n",
      "Iteration 134, loss = 0.32856249\n",
      "Iteration 135, loss = 0.32581755\n",
      "Iteration 136, loss = 0.32309028\n",
      "Iteration 137, loss = 0.32038097\n",
      "Iteration 138, loss = 0.31768989\n",
      "Iteration 139, loss = 0.31501727\n",
      "Iteration 140, loss = 0.31236332\n",
      "Iteration 141, loss = 0.30972820\n",
      "Iteration 142, loss = 0.30711205\n",
      "Iteration 143, loss = 0.30451495\n",
      "Iteration 144, loss = 0.30193698\n",
      "Iteration 145, loss = 0.29937813\n",
      "Iteration 146, loss = 0.29683840\n",
      "Iteration 147, loss = 0.29431771\n",
      "Iteration 148, loss = 0.29181595\n",
      "Iteration 149, loss = 0.28933297\n",
      "Iteration 150, loss = 0.28686857\n",
      "Iteration 151, loss = 0.28442249\n",
      "Iteration 152, loss = 0.28199444\n",
      "Iteration 153, loss = 0.27958405\n",
      "Iteration 154, loss = 0.27719093\n",
      "Iteration 155, loss = 0.27481463\n",
      "Iteration 156, loss = 0.27245465\n",
      "Iteration 157, loss = 0.27011044\n",
      "Iteration 158, loss = 0.26778142\n",
      "Iteration 159, loss = 0.26546699\n",
      "Iteration 160, loss = 0.26316649\n",
      "Iteration 161, loss = 0.26087926\n",
      "Iteration 162, loss = 0.25860463\n",
      "Iteration 163, loss = 0.25634193\n",
      "Iteration 164, loss = 0.25409048\n",
      "Iteration 165, loss = 0.25184967\n",
      "Iteration 166, loss = 0.24961888\n",
      "Iteration 167, loss = 0.24739758\n",
      "Iteration 168, loss = 0.24518527\n",
      "Iteration 169, loss = 0.24298156\n",
      "Iteration 170, loss = 0.24078610\n",
      "Iteration 171, loss = 0.23859867\n",
      "Iteration 172, loss = 0.23641911\n",
      "Iteration 173, loss = 0.23424740\n",
      "Iteration 174, loss = 0.23208359\n",
      "Iteration 175, loss = 0.22992784\n",
      "Iteration 176, loss = 0.22778043\n",
      "Iteration 177, loss = 0.22564168\n",
      "Iteration 178, loss = 0.22351204\n",
      "Iteration 179, loss = 0.22139198\n",
      "Iteration 180, loss = 0.21928207\n",
      "Iteration 181, loss = 0.21718289\n",
      "Iteration 182, loss = 0.21509508\n",
      "Iteration 183, loss = 0.21301928\n",
      "Iteration 184, loss = 0.21095617\n",
      "Iteration 185, loss = 0.20890645\n",
      "Iteration 186, loss = 0.20687084\n",
      "Iteration 187, loss = 0.20485004\n",
      "Iteration 188, loss = 0.20284481\n",
      "Iteration 189, loss = 0.20085589\n",
      "Iteration 190, loss = 0.19888405\n",
      "Iteration 191, loss = 0.19693005\n",
      "Iteration 192, loss = 0.19499466\n",
      "Iteration 193, loss = 0.19307864\n",
      "Iteration 194, loss = 0.19118276\n",
      "Iteration 195, loss = 0.18930773\n",
      "Iteration 196, loss = 0.18745426\n",
      "Iteration 197, loss = 0.18562303\n",
      "Iteration 198, loss = 0.18381466\n",
      "Iteration 199, loss = 0.18202973\n",
      "Iteration 200, loss = 0.18026876\n",
      "Iteration 201, loss = 0.17853223\n",
      "Iteration 202, loss = 0.17682055\n",
      "Iteration 203, loss = 0.17513407\n",
      "Iteration 204, loss = 0.17347310\n",
      "Iteration 205, loss = 0.17183786\n",
      "Iteration 206, loss = 0.17022855\n",
      "Iteration 207, loss = 0.16864529\n",
      "Iteration 208, loss = 0.16708816\n",
      "Iteration 209, loss = 0.16555718\n",
      "Iteration 210, loss = 0.16405235\n",
      "Iteration 211, loss = 0.16257358\n",
      "Iteration 212, loss = 0.16112079\n",
      "Iteration 213, loss = 0.15969383\n",
      "Iteration 214, loss = 0.15829251\n",
      "Iteration 215, loss = 0.15691663\n",
      "Iteration 216, loss = 0.15556596\n",
      "Iteration 217, loss = 0.15424023\n",
      "Iteration 218, loss = 0.15293915\n",
      "Iteration 219, loss = 0.15166243\n",
      "Iteration 220, loss = 0.15040973\n",
      "Iteration 221, loss = 0.14918071\n",
      "Iteration 222, loss = 0.14797504\n",
      "Iteration 223, loss = 0.14679235\n",
      "Iteration 224, loss = 0.14563226\n",
      "Iteration 225, loss = 0.14449441\n",
      "Iteration 226, loss = 0.14337840\n",
      "Iteration 227, loss = 0.14228386\n",
      "Iteration 228, loss = 0.14121039\n",
      "Iteration 229, loss = 0.14015760\n",
      "Iteration 230, loss = 0.13912510\n",
      "Iteration 231, loss = 0.13811249\n",
      "Iteration 232, loss = 0.13711939\n",
      "Iteration 233, loss = 0.13614540\n",
      "Iteration 234, loss = 0.13519013\n",
      "Iteration 235, loss = 0.13425321\n",
      "Iteration 236, loss = 0.13333425\n",
      "Iteration 237, loss = 0.13243288\n",
      "Iteration 238, loss = 0.13154872\n",
      "Iteration 239, loss = 0.13068140\n",
      "Iteration 240, loss = 0.12983057\n",
      "Iteration 241, loss = 0.12899586\n",
      "Iteration 242, loss = 0.12817694\n",
      "Iteration 243, loss = 0.12737344\n",
      "Iteration 244, loss = 0.12658503\n",
      "Iteration 245, loss = 0.12581138\n",
      "Iteration 246, loss = 0.12505217\n",
      "Iteration 247, loss = 0.12430706\n",
      "Iteration 248, loss = 0.12357575\n",
      "Iteration 249, loss = 0.12285793\n",
      "Iteration 250, loss = 0.12215330\n",
      "Iteration 251, loss = 0.12146156\n",
      "Iteration 252, loss = 0.12078243\n",
      "Iteration 253, loss = 0.12011561\n",
      "Iteration 254, loss = 0.11946085\n",
      "Iteration 255, loss = 0.11881785\n",
      "Iteration 256, loss = 0.11818637\n",
      "Iteration 257, loss = 0.11756614\n",
      "Iteration 258, loss = 0.11695691\n",
      "Iteration 259, loss = 0.11635843\n",
      "Iteration 260, loss = 0.11577048\n",
      "Iteration 261, loss = 0.11519280\n",
      "Iteration 262, loss = 0.11462517\n",
      "Iteration 263, loss = 0.11406737\n",
      "Iteration 264, loss = 0.11351918\n",
      "Iteration 265, loss = 0.11298039\n",
      "Iteration 266, loss = 0.11245079\n",
      "Iteration 267, loss = 0.11193018\n",
      "Iteration 268, loss = 0.11141835\n",
      "Iteration 269, loss = 0.11091513\n",
      "Iteration 270, loss = 0.11042031\n",
      "Iteration 271, loss = 0.10993372\n",
      "Iteration 272, loss = 0.10945518\n",
      "Iteration 273, loss = 0.10898451\n",
      "Iteration 274, loss = 0.10852155\n",
      "Iteration 275, loss = 0.10806612\n",
      "Iteration 276, loss = 0.10761807\n",
      "Iteration 277, loss = 0.10717724\n",
      "Iteration 278, loss = 0.10674347\n",
      "Iteration 279, loss = 0.10631662\n",
      "Iteration 280, loss = 0.10589653\n",
      "Iteration 281, loss = 0.10548307\n",
      "Iteration 282, loss = 0.10507610\n",
      "Iteration 283, loss = 0.10467549\n",
      "Iteration 284, loss = 0.10428109\n",
      "Iteration 285, loss = 0.10389278\n",
      "Iteration 286, loss = 0.10351043\n",
      "Iteration 287, loss = 0.10313393\n",
      "Iteration 288, loss = 0.10276315\n",
      "Iteration 289, loss = 0.10239798\n",
      "Iteration 290, loss = 0.10203830\n",
      "Iteration 291, loss = 0.10168399\n",
      "Iteration 292, loss = 0.10133496\n",
      "Iteration 293, loss = 0.10099110\n",
      "Iteration 294, loss = 0.10065229\n",
      "Iteration 295, loss = 0.10031845\n",
      "Iteration 296, loss = 0.09998947\n",
      "Iteration 297, loss = 0.09966526\n",
      "Iteration 298, loss = 0.09934573\n",
      "Iteration 299, loss = 0.09903077\n",
      "Iteration 300, loss = 0.09872031\n",
      "Iteration 301, loss = 0.09841425\n",
      "Iteration 302, loss = 0.09811251\n",
      "Iteration 303, loss = 0.09781501\n",
      "Iteration 304, loss = 0.09752166\n",
      "Iteration 305, loss = 0.09723239\n",
      "Iteration 306, loss = 0.09694712\n",
      "Iteration 307, loss = 0.09666577\n",
      "Iteration 308, loss = 0.09638827\n",
      "Iteration 309, loss = 0.09611455\n",
      "Iteration 310, loss = 0.09584453\n",
      "Iteration 311, loss = 0.09557814\n",
      "Iteration 312, loss = 0.09531533\n",
      "Iteration 313, loss = 0.09505602\n",
      "Iteration 314, loss = 0.09480015\n",
      "Iteration 315, loss = 0.09454765\n",
      "Iteration 316, loss = 0.09429847\n",
      "Iteration 317, loss = 0.09405254\n",
      "Iteration 318, loss = 0.09380980\n",
      "Iteration 319, loss = 0.09357020\n",
      "Iteration 320, loss = 0.09333369\n",
      "Iteration 321, loss = 0.09310019\n",
      "Iteration 322, loss = 0.09286967\n",
      "Iteration 323, loss = 0.09264207\n",
      "Iteration 324, loss = 0.09241733\n",
      "Iteration 325, loss = 0.09219541\n",
      "Iteration 326, loss = 0.09197626\n",
      "Iteration 327, loss = 0.09175983\n",
      "Iteration 328, loss = 0.09154607\n",
      "Iteration 329, loss = 0.09133493\n",
      "Iteration 330, loss = 0.09112637\n",
      "Iteration 331, loss = 0.09092035\n",
      "Iteration 332, loss = 0.09071682\n",
      "Iteration 333, loss = 0.09051573\n",
      "Iteration 334, loss = 0.09031706\n",
      "Iteration 335, loss = 0.09012075\n",
      "Iteration 336, loss = 0.08992677\n",
      "Iteration 337, loss = 0.08973507\n",
      "Iteration 338, loss = 0.08954563\n",
      "Iteration 339, loss = 0.08935839\n",
      "Iteration 340, loss = 0.08917333\n",
      "Iteration 341, loss = 0.08899041\n",
      "Iteration 342, loss = 0.08880959\n",
      "Iteration 343, loss = 0.08863084\n",
      "Iteration 344, loss = 0.08845412\n",
      "Iteration 345, loss = 0.08827941\n",
      "Iteration 346, loss = 0.08810666\n",
      "Iteration 347, loss = 0.08793585\n",
      "Iteration 348, loss = 0.08776695\n",
      "Iteration 349, loss = 0.08759992\n",
      "Iteration 350, loss = 0.08743473\n",
      "Iteration 351, loss = 0.08727136\n",
      "Iteration 352, loss = 0.08710977\n",
      "Iteration 353, loss = 0.08694995\n",
      "Iteration 354, loss = 0.08679185\n",
      "Iteration 355, loss = 0.08663545\n",
      "Iteration 356, loss = 0.08648073\n",
      "Iteration 357, loss = 0.08632766\n",
      "Iteration 358, loss = 0.08617621\n",
      "Iteration 359, loss = 0.08602636\n",
      "Iteration 360, loss = 0.08587808\n",
      "Iteration 361, loss = 0.08573136\n",
      "Iteration 362, loss = 0.08558615\n",
      "Iteration 363, loss = 0.08544245\n",
      "Iteration 364, loss = 0.08530023\n",
      "Iteration 365, loss = 0.08515946\n",
      "Iteration 366, loss = 0.08502012\n",
      "Iteration 367, loss = 0.08488220\n",
      "Iteration 368, loss = 0.08474567\n",
      "Iteration 369, loss = 0.08461051\n",
      "Iteration 370, loss = 0.08447669\n",
      "Iteration 371, loss = 0.08434421\n",
      "Iteration 372, loss = 0.08421303\n",
      "Iteration 373, loss = 0.08408314\n",
      "Iteration 374, loss = 0.08395452\n",
      "Iteration 375, loss = 0.08382716\n",
      "Iteration 376, loss = 0.08370103\n",
      "Iteration 377, loss = 0.08357611\n",
      "Iteration 378, loss = 0.08345238\n",
      "Iteration 379, loss = 0.08332984\n",
      "Iteration 380, loss = 0.08320846\n",
      "Iteration 381, loss = 0.08308823\n",
      "Iteration 382, loss = 0.08296912\n",
      "Iteration 383, loss = 0.08285112\n",
      "Iteration 384, loss = 0.08273423\n",
      "Iteration 385, loss = 0.08261841\n",
      "Iteration 386, loss = 0.08250366\n",
      "Iteration 387, loss = 0.08238995\n",
      "Iteration 388, loss = 0.08227728\n",
      "Iteration 389, loss = 0.08216564\n",
      "Iteration 390, loss = 0.08205499\n",
      "Iteration 391, loss = 0.08194534\n",
      "Iteration 392, loss = 0.08183667\n",
      "Iteration 393, loss = 0.08172896\n",
      "Iteration 394, loss = 0.08162220\n",
      "Iteration 395, loss = 0.08151638\n",
      "Iteration 396, loss = 0.08141148\n",
      "Iteration 397, loss = 0.08130749\n",
      "Iteration 398, loss = 0.08120441\n",
      "Iteration 399, loss = 0.08110220\n",
      "Iteration 400, loss = 0.08100088\n",
      "Iteration 401, loss = 0.08090041\n",
      "Iteration 402, loss = 0.08080080\n",
      "Iteration 403, loss = 0.08070202\n",
      "Iteration 404, loss = 0.08060407\n",
      "Training loss did not improve more than tol=0.000100 for two consecutive epochs. Stopping.\n",
      "Saída da rede:\t [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n",
      " 0 1 2 2 0 2 2 1]\n",
      "Saída desejada:\t [0 1 1 0 2 1 2 0 0 2 1 0 2 1 1 0 1 1 0 0 1 1 1 0 2 1 0 0 1 2 1 2 1 2 2 0 1\n",
      " 0 1 2 2 0 2 2 1]\n",
      "Score:  1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "x, y = iris.data, iris.target\n",
    "\n",
    "mlp = MLPClassifier(solver='adam', alpha=0.0001, hidden_layer_sizes=(5), random_state=42, \n",
    "                    learning_rate='constant', learning_rate_init=0.01, max_iter=500, activation='logistic', \n",
    "                    momentum=0.9, verbose=True, tol=0.0001)\n",
    "\n",
    "x_treino, x_teste, y_treino, y_teste = train_test_split(x, y, test_size=0.3, random_state=1)\n",
    "mlp.fit(x_treino, y_treino)\n",
    "\n",
    "saidas = mlp.predict(x_teste)\n",
    "\n",
    "print('Saída da rede:\\t', saidas)\n",
    "print('Saída desejada:\\t', y_teste)\n",
    "print('Score: ', mlp.score(x_teste, y_teste))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
